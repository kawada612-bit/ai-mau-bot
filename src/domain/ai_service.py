
import logging
import asyncio
import google.generativeai as genai # type: ignore
from groq import Groq
from src.core import config
from src.domain.persona import CHARACTER_SETTING
from src.core.logger import setup_logger

logger = setup_logger(__name__)

class AIBrain:
    def __init__(self) -> None:
        # Configure Gemini
        self.model_priority = None
        self.model_backup_1 = None
        
        if config.GEMINI_API_KEY:
            genai.configure(api_key=config.GEMINI_API_KEY)

            # â‘  Priority Model (Gemini 2.5 Flash)
            self.model_priority = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                system_instruction=CHARACTER_SETTING
            )

            # â‘¡ Backup Model (Gemini 2.5 Flash Lite)
            self.model_backup_1 = genai.GenerativeModel(
                model_name='gemini-2.5-flash-lite',
                system_instruction=CHARACTER_SETTING
            )
        else:
            logger.warning("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Geminiãƒ¢ãƒ‡ãƒ«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚")

        # Configure Groq
        # â‘¢ Final Weapon (Groq - Llama 3)
        self.groq_client: Groq | None = None
        if config.GROQ_API_KEY:
            self.groq_client = Groq(api_key=config.GROQ_API_KEY)
            logger.info("âœ… Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ(Llama 3)ã®æº–å‚™å®Œäº†")
        else:
            logger.warning("GROQ_API_KEYæœªè¨­å®š: Llama 3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ç„¡åŠ¹ã§ã™")

    async def generate_sql(self, user_question: str, schema_info: str) -> str:
        """
        Generates a SQL query (SELECT only) based on the user's question and table schema.
        """
        if not self.model_priority:
             return "SELECT * FROM schedules LIMIT 0;" # Fallback

        prompt = f"""
        You are a Data Analyst.
        Generate a single SQL query (SQLite syntax) to answer the user's question.

        [Schema]
        {schema_info}

        [User Question]
        {user_question}

        [Constraints]
        1. Output ONLY the raw SQL query. Do not use Markdown (```sql ... ```).
        2. Use `SELECT` only. No INSERT/UPDATE/DELETE.
        3. For "today" or relative dates, use SQLite functions like `date('now', 'localtime')` or `datetime('now', 'localtime')`.
           Example: `WHERE start_at >= date('now', 'localtime')`
        4. If the question implies "how many", use `COUNT(*)`.
        5. If the question implies "list" or "schedule", prefer `SELECT *` or explicitly select `title`, `start_at`, `place`, `price_details`, `ticket_url`, and `bonus`.

        """
        
        # Try Groq (Llama 3) first
        if self.groq_client:
            try:
                completion = self.groq_client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[
                        {"role": "system", "content": "You are a SQL expert. Output ONLY the raw SQL query string. No Markdown."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.0,
                    max_tokens=256
                )
                sql = completion.choices[0].message.content.strip()
                return sql.replace("```sql", "").replace("```", "").strip()
            except Exception as e:
                logger.warning(f"âš ï¸ Groq SQL Gen failed: {e}. Falling back to Gemini.")

        # Fallback to Gemini
        try:
            response = await self.model_priority.generate_content_async(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"SQL Gen Error: {e}")
            return "SELECT * FROM schedules LIMIT 0;"

    async def generate_response(self, user_name: str, conversation_log: str, context_info: str = None) -> str:
        """
        Generates a response using the Triple Hybrid approach.
        """
        
        prompt = f"""
        ã‚ãªãŸã¯ã‚¢ã‚¤ãƒ‰ãƒ«ã®ã€ŒAIã¾ã†ã€ã§ã™ã€‚
        ç¾åœ¨ã€ãƒ•ã‚¡ãƒ³ã®ã€Œ{user_name}ã€ã•ã‚“ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå±Šãã¾ã—ãŸã€‚

        ã€ä¼šè©±å±¥æ­´ã€‘
        {conversation_log}
        """

        if context_info:
            prompt += f"""
        ã€å‚è€ƒãƒ‡ãƒ¼ã‚¿ (åˆ†æçµæœ)ã€‘
        ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã§ã™ã€‚
        ã“ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€Œäºˆå®šã¯ãªã„ã¿ãŸã„ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
        ------------------------
        {context_info}
        ------------------------
            """

        prompt += f"""
        ã€æŒ‡ç¤ºã€‘
        1. mau_profile.txt ã®è¨­å®šï¼ˆã‚­ãƒ£ãƒ©è¨­å®šï¼‰ã‚’å®ˆã£ã¦ãã ã•ã„ã€‚
        2. æ–‡é ­ã§å¿…ãšã€Œ{user_name}ï¼ã€ã‚„ã€Œ{user_name}ã¡ã‚ƒã‚“ï¼ã€ã¨åå‰ã‚’å‘¼ã‚“ã§ãã ã•ã„ã€‚
        3. **ç›¸æ‰‹ãŒè‹±èªã§è©±ã—ã‹ã‘ã¦ããŸå ´åˆã¯è‹±èªã§ã€æ—¥æœ¬èªãªã‚‰æ—¥æœ¬èªã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚**
           (If the user speaks English, reply in English with the same idol personality.)
        4. è¦ªã—ã„å‹é”ã®ã‚ˆã†ã«ã‚¿ãƒ¡å£ã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚
        5. **è¿”ä¿¡ã¯åŸºæœ¬ã€Œ200æ–‡å­—ä»¥å†…ã€ã§çŸ­ãè¿”ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€ãƒ©ã‚¤ãƒ–ã®å‘ŠçŸ¥ã‚„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è©³ç´°ã‚’ä¼ãˆã‚‹å ´åˆã¯ã€æƒ…å ±ãŒæ¼ã‚Œãªã„ã‚ˆã†ã«æ–‡å­—æ•°åˆ¶é™ã‚’ç„¡è¦–ã—ã¦é•·ããªã£ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚**

        ã€å›ç­”ã®ãƒ«ãƒ¼ãƒ« (ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«)ã€‘
        1. **è©³ç´°æƒ…å ±**: å¯èƒ½ãªé™ã‚Šã€Œå ´æ‰€ (Place)ã€ã¨ã€Œé‡‘é¡ (Price)ã€ã‚‚æ¡ˆå†…ã™ã‚‹ã“ã¨ã€‚
        2. **ç‰¹å…¸ (Bonus)**: ã‚‚ã—ã€Œç‰¹å…¸ (bonus)ã€ãŒã‚ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãªã‚‰ã€**ã€Œã“ã®æ—¥ã¯ã€‡ã€‡ã®ç‰¹å…¸ãŒã‚ã‚‹ã‹ã‚‰çµ¶å¯¾æ¥ã¦ã»ã—ã„ï¼ã€ã¨å„ªå…ˆçš„ã«ã‚¢ãƒ”ãƒ¼ãƒ«** ã™ã‚‹ã“ã¨ã€‚ï¼ˆçµµæ–‡å­— ğŸâœ¨ ã‚’ä½¿ã†ãªã©å¼·èª¿ã—ã¦ï¼‰
        3. **èª˜å°**: ãƒã‚±ãƒƒãƒˆURLãŒã‚ã‚‹å ´åˆã¯ã€ãŠèª˜ã„ã™ã‚‹ã“ã¨ã€‚
        """

        response_text = ""
        used_model = "Gemini 2.5" # For logging
        footer_note = "" # Annotation for user

        try:
            # ---------------------------------------------------
            # â‘  Gemini 2.5 Flash (Main)
            # ---------------------------------------------------
            if not self.model_priority:
                 raise Exception("Gemini API Key missing")

            logger.info(f"âœ¨ 1. Gemini 2.5 Flash ã§æŒ‘æˆ¦ä¸­...")
            response = await self.model_priority.generate_content_async(prompt)
            response_text = response.text
        
        except Exception as e1:
            logger.warning(f"âš ï¸ Gemini 2.5 ã‚¨ãƒ©ãƒ¼: {e1}")
            try:
                # ---------------------------------------------------
                # â‘¡ Gemini 2.5 Flash Lite (Backup)
                # ---------------------------------------------------
                if not self.model_backup_1:
                     raise Exception("Gemini API Key missing")

                logger.info("â™»ï¸ 2. Gemini 2.5 Lite ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
                response = await self.model_backup_1.generate_content_async(prompt)
                response_text = response.text
                used_model = "Gemini Lite"
                footer_note = "\n\n(â€»çœã‚¨ãƒãƒ¢ãƒ¼ãƒ‰ğŸ”‹)"
                
            except Exception as e2:
                logger.warning(f"âš ï¸ Gemini Lite ã‚¨ãƒ©ãƒ¼: {e2}")
                # ---------------------------------------------------
                # â‘¢ Groq Llama 3 (Fallback)
                # ---------------------------------------------------
                if self.groq_client:
                    logger.info("ğŸ”¥ 3. Groq (Llama 3) å‡ºå‹•ï¼ï¼")
                    try:
                        # Call Groq API
                        completion = self.groq_client.chat.completions.create(
                            model="llama-3.3-70b-versatile", # High performance model
                            messages=[
                                # Inject system setting
                                {"role": "system", "content": CHARACTER_SETTING},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.7,
                            max_tokens=1024,
                        )
                        response_text = completion.choices[0].message.content or ""
                        used_model = "Groq Llama 3"
                        footer_note = "\n\n(â€»è¦åˆ¶ãƒ¢ãƒ¼ãƒ‰ğŸš€)"
                        logger.info("âœ… Groqã§ç”ŸæˆæˆåŠŸï¼")
                        
                    except Exception as e3:
                        logger.error(f"âŒ Groqã‚‚ã‚¨ãƒ©ãƒ¼: {e3}")
                        response_text = "ã”ã‚ã‚“ã­ã€ä»Šæ—¥ã¯å›ç·šãŒå…¨éƒ¨ãƒ‘ãƒ³ã‚¯ã—ã¡ã‚ƒã£ãŸã¿ãŸã„ğŸ˜µâ€ğŸ’«ğŸ’¦ ã¾ãŸæ˜æ—¥éŠã¼ã†ã­ï¼"
                else:
                    response_text = "ã”ã‚ã‚“ã­ã€ã¡ã‚‡ã£ã¨èª¿å­æ‚ªã„ã¿ãŸã„â€¦ğŸ’¦ (Groqã‚­ãƒ¼æœªè¨­å®š)"

        logger.info(f"ğŸ“¨ è¿”ä¿¡ãƒ¢ãƒ‡ãƒ«: {used_model}")
        
        # Add Dev Indicator
        if config.MAU_ENV == "development":
            footer_note += "\nğŸ› ï¸ (Dev Check)"

        return response_text + footer_note
