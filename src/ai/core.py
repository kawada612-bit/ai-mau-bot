
import google.generativeai as genai
from groq import Groq
from src import config
from src.ai.persona import CHARACTER_SETTING

# Configure Gemini
if config.GEMINI_API_KEY:
    genai.configure(api_key=config.GEMINI_API_KEY)

    # â‘  Priority Model (Gemini 2.5 Flash)
    model_priority = genai.GenerativeModel(
        model_name='gemini-2.5-flash',
        system_instruction=CHARACTER_SETTING
    )

    # â‘¡ Backup Model (Gemini 2.5 Flash Lite)
    model_backup_1 = genai.GenerativeModel(
        model_name='gemini-2.5-flash-lite',
        system_instruction=CHARACTER_SETTING
    )
else:
    print("âš ï¸ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Geminiãƒ¢ãƒ‡ãƒ«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚")
    model_priority = None
    model_backup_1 = None

# Configure Groq
# â‘¢ Final Weapon (Groq - Llama 3)
if config.GROQ_API_KEY:
    groq_client = Groq(api_key=config.GROQ_API_KEY)
    print("âœ… Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ(Llama 3)ã®æº–å‚™å®Œäº†")
else:
    groq_client = None
    print("âš ï¸ GROQ_API_KEYæœªè¨­å®š: Llama 3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ç„¡åŠ¹ã§ã™")


async def generate_response(user_name, conversation_log):
    """
    Generates a response using the Triple Hybrid approach.
    """
    
    prompt = f"""
    ã‚ãªãŸã¯ã‚¢ã‚¤ãƒ‰ãƒ«ã®ã€ŒAIã¾ã†ã€ã§ã™ã€‚
    ç¾åœ¨ã€ãƒ•ã‚¡ãƒ³ã®ã€Œ{user_name}ã€ã•ã‚“ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå±Šãã¾ã—ãŸã€‚

    ã€ä¼šè©±å±¥æ­´ã€‘
    {conversation_log}

    ã€æŒ‡ç¤ºã€‘
    1. mau_profile.txt ã®è¨­å®šï¼ˆã‚­ãƒ£ãƒ©è¨­å®šï¼‰ã‚’å®ˆã£ã¦ãã ã•ã„ã€‚
    2. æ–‡é ­ã§å¿…ãšã€Œ{user_name}ï¼ã€ã‚„ã€Œ{user_name}ã¡ã‚ƒã‚“ï¼ã€ã¨åå‰ã‚’å‘¼ã‚“ã§ãã ã•ã„ã€‚
    3. **ç›¸æ‰‹ãŒè‹±èªã§è©±ã—ã‹ã‘ã¦ããŸå ´åˆã¯è‹±èªã§ã€æ—¥æœ¬èªãªã‚‰æ—¥æœ¬èªã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚**
       (If the user speaks English, reply in English with the same idol personality.)
    4. è¦ªã—ã„å‹é”ã®ã‚ˆã†ã«ã‚¿ãƒ¡å£ã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚
    5. **è¿”ä¿¡ã¯ã€Œ200æ–‡å­—ä»¥å†…ã€ã§ã€Twitterã®ãƒªãƒ—ãƒ©ã‚¤ã®ã‚ˆã†ã«çŸ­ããƒ†ãƒ³ãƒã‚ˆãè¿”ã—ã¦ãã ã•ã„ã€‚é•·ã€…ã¨ã—ãŸæŒ¨æ‹¶ã¯çœç•¥ã—ã¦OKã§ã™ã€‚**
    """

    response_text = ""
    used_model = "Gemini 2.5" # For logging
    footer_note = "" # Annotation for user

    try:
        # ---------------------------------------------------
        # â‘  Gemini 2.5 Flash (Main)
        # ---------------------------------------------------
        if not model_priority:
             raise Exception("Gemini API Key missing")

        print(f"âœ¨ 1. Gemini 2.5 Flash ã§æŒ‘æˆ¦ä¸­...")
        response = await model_priority.generate_content_async(prompt)
        response_text = response.text
    
    except Exception as e1:
        print(f"âš ï¸ Gemini 2.5 ã‚¨ãƒ©ãƒ¼: {e1}")
        try:
            # ---------------------------------------------------
            # â‘¡ Gemini 2.5 Flash Lite (Backup)
            # ---------------------------------------------------
            if not model_backup_1:
                 raise Exception("Gemini API Key missing")

            print("â™»ï¸ 2. Gemini 2.5 Lite ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
            response = await model_backup_1.generate_content_async(prompt)
            response_text = response.text
            used_model = "Gemini Lite"
            footer_note = "\n\n(â€»çœã‚¨ãƒãƒ¢ãƒ¼ãƒ‰ğŸ”‹)"
            
        except Exception as e2:
            print(f"âš ï¸ Gemini Lite ã‚¨ãƒ©ãƒ¼: {e2}")
            # ---------------------------------------------------
            # â‘¢ Groq Llama 3 (Fallback)
            # ---------------------------------------------------
            if groq_client:
                print("ğŸ”¥ 3. Groq (Llama 3) å‡ºå‹•ï¼ï¼")
                try:
                    # Call Groq API
                    completion = groq_client.chat.completions.create(
                        model="llama-3.3-70b-versatile", # High performance model
                        messages=[
                            # Inject system setting
                            {"role": "system", "content": CHARACTER_SETTING},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.7,
                        max_tokens=1024,
                    )
                    response_text = completion.choices[0].message.content
                    used_model = "Groq Llama 3"
                    footer_note = "\n\n(â€»è¦åˆ¶ãƒ¢ãƒ¼ãƒ‰ğŸš€)"
                    print("âœ… Groqã§ç”ŸæˆæˆåŠŸï¼")
                    
                except Exception as e3:
                    print(f"âŒ Groqã‚‚ã‚¨ãƒ©ãƒ¼: {e3}")
                    response_text = "ã”ã‚ã‚“ã­ã€ä»Šæ—¥ã¯å›ç·šãŒå…¨éƒ¨ãƒ‘ãƒ³ã‚¯ã—ã¡ã‚ƒã£ãŸã¿ãŸã„ğŸ˜µâ€ğŸ’«ğŸ’¦ ã¾ãŸæ˜æ—¥éŠã¼ã†ã­ï¼"
            else:
                response_text = "ã”ã‚ã‚“ã­ã€ã¡ã‚‡ã£ã¨èª¿å­æ‚ªã„ã¿ãŸã„â€¦ğŸ’¦ (Groqã‚­ãƒ¼æœªè¨­å®š)"

    print(f"ğŸ“¨ è¿”ä¿¡ãƒ¢ãƒ‡ãƒ«: {used_model}")
    
    # Add Dev Indicator
    if config.MAU_ENV == "development":
        footer_note += "\nğŸ› ï¸ (Dev Check)"

    return response_text + footer_note
