
import time
import json
import sys
import argparse
from datetime import datetime, timedelta, timezone
from dateutil.relativedelta import relativedelta
from typing import Optional
from playwright.sync_api import sync_playwright
from supabase import create_client
from groq import Groq
from src.core import config
from src.core.logger import setup_logger

logger = setup_logger(__name__)

# å®šæ•°
TIMETREE_BASE_URL: str = "https://timetreeapp.com/public_calendars/lollipop_1116"

# GroqåˆæœŸåŒ–
groq_client: Optional[Groq] = None
if config.GROQ_API_KEY:
    groq_client = Groq(api_key=config.GROQ_API_KEY)

def check_env_vars() -> bool:
    """ç’°å¢ƒå¤‰æ•°ã®è¨­å®šçŠ¶æ³ã‚’ç¢ºèª"""
    logger.info("--- âš™ï¸ è¨­å®šãƒã‚§ãƒƒã‚¯ ---")
    logger.info(f"Supabase URL : {'âœ… OK' if config.SUPABASE_URL else 'âŒ Missing'}")
    logger.info(f"Supabase Key : {'âœ… OK' if config.SUPABASE_KEY else 'âŒ Missing'}")
    logger.info(f"Groq Key     : {'âœ… OK' if config.GROQ_API_KEY else 'âŒ Missing'}")
    logger.info("-----------------------")
    return bool(config.SUPABASE_URL and config.SUPABASE_KEY)

def extract_details_with_groq(title: str, date_str: str, note: str) -> dict:
    """Groq (Llama 3) ã§ãƒ¡ãƒ¢æ¬„ã‹ã‚‰è©³ç´°æƒ…å ±ï¼ˆæ™‚é–“ã€å ´æ‰€ã€ãƒã‚±ãƒƒãƒˆã€æ–™é‡‘ã€ç‰¹å…¸ï¼‰ã‚’æŠ½å‡º"""
    if not note or not groq_client: return {}
    
    prompt = f"""
    You are a precise data extraction engine.
    Extract information from the text **exactly as it appears** in the source.

    [Input]
    Date: {date_str}
    Title: {title}
    Note: {note}

    [Rules]
    1. Output JSON ONLY.
    2. **ticket_url**: Extract ticket links. Look for domains like 'livepocket.jp', 't.livepocket.jp', 't-dv.com', 'tiget.net' even if 'https://' is missing. If missing protocol, prepend 'https://'.
    3. **price**: Extract ticket price details. KEEP ORIGINAL TEXT. Do NOT translate 'Â¥' to 'å…ƒ' or 'Yuan'. Do NOT change currency symbols.
    4. **bonus**: Extract text related to 'ç‰¹å…¸', 'æ‹›å¾…', 'å†™ãƒ¡', 'ãƒã‚§ã‚­', 'å‹•ç”»', 'ãã˜', 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ'.
    5. **place**: Venue name (e.g. "SHIBUYA CYCLONE").
    
    6. **start_at/end_at**:
       - Extract START/END times (ISO 8601: YYYY-MM-DDTHH:MM:SS+09:00).
       - Handle "1040" as "10:40".
       - If "OPEN" and "START" exist, use "START" for start_at. If only "OPEN", use "OPEN".
       - If time is "TBA" or unknown, return null for times.

    Output Schema:
    {{
       "start_at": "ISO string or null",
       "end_at": "ISO string or null",
       "place": "string or null",
       "ticket_url": "string or null",
       "price": "string or null",
       "bonus": "string or null"
    }}
    """
    
    try:
        completion = groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "Output JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        content = completion.choices[0].message.content or "{}"
        return json.loads(content)
    except Exception as e:
        logger.warning(f"AIè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def fetch_and_sync(dry_run: bool = False) -> None:
    if not check_env_vars(): return
    
    logger.info(f"ğŸš€ åŒæœŸãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ (ãƒ¢ãƒ¼ãƒ‰: {'Dry Run' if dry_run else 'é€šå¸¸å®Ÿè¡Œ'})...")
    all_events = {}

    with sync_playwright() as p:
        logger.info("ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­...")
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        def handle_response(response):
            if "public_events" in response.url and response.status == 200:
                try:
                    data = response.json()
                    events = data.get("public_events", [])
                    for e in events:
                        all_events[e["id"]] = e
                except: pass

        page.on("response", handle_response)

        # ä»Šæœˆã‹ã‚‰å‘ã“ã†4ãƒ¶æœˆåˆ†ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«å·¡å›
        today = datetime.now()
        for i in range(4):
            target_date = today + relativedelta(months=i)
            date_param = target_date.strftime("%Y-%m-01")
            url = f"{TIMETREE_BASE_URL}?monthly={date_param}"
            
            logger.info(f"ğŸ”„ å·¡å›: {date_param} ...")
            try:
                page.goto(url, wait_until="networkidle")
                page.wait_for_timeout(1500)
            except Exception as e:
                logger.warning(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")

        browser.close()

    if not all_events:
        logger.warning("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨ä¿å­˜
    upsert_data = []
    events_list = list(all_events.values())
    logger.info(f"ğŸ“¦ åˆè¨ˆ {len(events_list)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ä¸­...")

    for event in events_list:
        try:
            source_id = str(event["id"])
            title = event.get("title", "")
            note = event.get("note", "")
            raw_start = event["start_at"] / 1000
            
            # æ›´æ–°æ—¥æ™‚ã®å–å¾—
            raw_updated_at = event.get("updated_at")
            if raw_updated_at:
                updated_at_dt = datetime.fromtimestamp(raw_updated_at / 1000, timezone.utc)
            else:
                updated_at_dt = datetime.now(timezone.utc)

            dt_obj = datetime.fromtimestamp(raw_start, timezone(timedelta(hours=9)))
            start_at = dt_obj.isoformat()
            end_at = None
            is_all_day = event.get("all_day", False)
            
            place = None
            ticket_url = None
            price_details = None
            bonus = None

            # AIè£œæ­£
            if note and groq_client:
                extracted = extract_details_with_groq(title, dt_obj.strftime('%Y-%m-%d'), note)
                
                ai_start = extracted.get("start_at")
                ai_end = extracted.get("end_at")
                place = extracted.get("place")
                ticket_url = extracted.get("ticket_url")
                price_details = extracted.get("price")
                bonus = extracted.get("bonus")

                if ai_start:
                    # æ—¥ä»˜æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                    original_date = dt_obj.date()
                    try:
                        ai_dt = datetime.fromisoformat(ai_start)
                        ai_date = ai_dt.date()
                        
                        if original_date != ai_date:
                            logger.warning(f"âš ï¸ AI Date Mismatch! Skipping AI result. Original: {original_date}, AI: {ai_date}")
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: AIçµæœã‚’ç ´æ£„ã—ã¦å…ƒã®æ™‚é–“ã‚’ä½¿ç”¨
                        else:
                            start_at = ai_start
                            is_all_day = False
                            if ai_end: end_at = ai_end
                            logger.info(f"  ğŸ¤– AIè§£ææˆåŠŸ: {title[:15]}... -> {ai_start} | ğŸ“ {place} | ğŸ« {price_details} | ğŸ {bonus}")
                            time.sleep(0.3)
                    except ValueError:
                        logger.warning(f"âš ï¸ AI returned invalid date format: {ai_start}")
                else:
                    logger.debug(f"  ğŸ¤– AIè§£æã‚¹ã‚­ãƒƒãƒ—: {title[:15]}...")

            upsert_data.append({
                "source_id": source_id,
                "title": title,
                "start_at": start_at,
                "end_at": end_at,
                "description": note,
                "url": event.get("url", ""),
                "image_url": None,
                "is_all_day": is_all_day,
                "updated_at": updated_at_dt.isoformat(),
                "place": place,
                "ticket_url": ticket_url,
                "price_details": price_details,
                "bonus": bonus
            })
        except Exception as e:
            logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")

    if upsert_data:
        if dry_run:
            logger.info(f"[Dry Run] Would upsert {len(upsert_data)} items:")
            logger.info(json.dumps(upsert_data, indent=2, default=str))
        else:
            try:
                if config.SUPABASE_URL and config.SUPABASE_KEY:
                    supabase = create_client(config.SUPABASE_URL, config.SUPABASE_KEY)
                    supabase.table("schedules").upsert(upsert_data, on_conflict="source_id").execute()
                    logger.info(f"âœ… åŒæœŸå®Œäº†ï¼ {len(upsert_data)} ä»¶ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
                else:
                    logger.error("Supabase config failed")
            except Exception as e:
                logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        logger.warning("âš ï¸ ä¿å­˜ãƒ‡ãƒ¼ã‚¿ãªã—")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Scheduler Worker")
    parser.add_argument("--dry-run", action="store_true", help="Perform a dry run without writing to DB")
    args = parser.parse_args()
    
    fetch_and_sync(dry_run=args.dry_run)
    sys.exit(0)
