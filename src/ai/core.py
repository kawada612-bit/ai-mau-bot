
import logging
import asyncio
import google.generativeai as genai # type: ignore
from groq import Groq
from src import config
from src.ai.persona import CHARACTER_SETTING
from src.logger import setup_logger

logger = setup_logger(__name__)

class AIBrain:
    def __init__(self) -> None:
        # Configure Gemini
        self.model_priority = None
        self.model_backup_1 = None
        
        if config.GEMINI_API_KEY:
            genai.configure(api_key=config.GEMINI_API_KEY)

            # â‘  Priority Model (Gemini 2.5 Flash)
            self.model_priority = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                system_instruction=CHARACTER_SETTING
            )

            # â‘¡ Backup Model (Gemini 2.5 Flash Lite)
            self.model_backup_1 = genai.GenerativeModel(
                model_name='gemini-2.5-flash-lite',
                system_instruction=CHARACTER_SETTING
            )
        else:
            logger.warning("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Geminiãƒ¢ãƒ‡ãƒ«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚")

        # Configure Groq
        # â‘¢ Final Weapon (Groq - Llama 3)
        self.groq_client: Groq | None = None
        if config.GROQ_API_KEY:
            self.groq_client = Groq(api_key=config.GROQ_API_KEY)
            logger.info("âœ… Groqã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ(Llama 3)ã®æº–å‚™å®Œäº†")
        else:
            logger.warning("GROQ_API_KEYæœªè¨­å®š: Llama 3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ç„¡åŠ¹ã§ã™")

    async def generate_response(self, user_name: str, conversation_log: str) -> str:
        """
        Generates a response using the Triple Hybrid approach.
        
        Args:
            user_name (str): The name of the user sending the message.
            conversation_log (str): The history of the conversation.

        Returns:
            str: The generated response text.
        """
        
        prompt = f"""
        ã‚ãªãŸã¯ã‚¢ã‚¤ãƒ‰ãƒ«ã®ã€ŒAIã¾ã†ã€ã§ã™ã€‚
        ç¾åœ¨ã€ãƒ•ã‚¡ãƒ³ã®ã€Œ{user_name}ã€ã•ã‚“ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå±Šãã¾ã—ãŸã€‚

        ã€ä¼šè©±å±¥æ­´ã€‘
        {conversation_log}

        ã€æŒ‡ç¤ºã€‘
        1. mau_profile.txt ã®è¨­å®šï¼ˆã‚­ãƒ£ãƒ©è¨­å®šï¼‰ã‚’å®ˆã£ã¦ãã ã•ã„ã€‚
        2. æ–‡é ­ã§å¿…ãšã€Œ{user_name}ï¼ã€ã‚„ã€Œ{user_name}ã¡ã‚ƒã‚“ï¼ã€ã¨åå‰ã‚’å‘¼ã‚“ã§ãã ã•ã„ã€‚
        3. **ç›¸æ‰‹ãŒè‹±èªã§è©±ã—ã‹ã‘ã¦ããŸå ´åˆã¯è‹±èªã§ã€æ—¥æœ¬èªãªã‚‰æ—¥æœ¬èªã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚**
           (If the user speaks English, reply in English with the same idol personality.)
        4. è¦ªã—ã„å‹é”ã®ã‚ˆã†ã«ã‚¿ãƒ¡å£ã§è¿”ä¿¡ã—ã¦ãã ã•ã„ã€‚
        5. **è¿”ä¿¡ã¯ã€Œ200æ–‡å­—ä»¥å†…ã€ã§ã€Twitterã®ãƒªãƒ—ãƒ©ã‚¤ã®ã‚ˆã†ã«çŸ­ããƒ†ãƒ³ãƒã‚ˆãè¿”ã—ã¦ãã ã•ã„ã€‚é•·ã€…ã¨ã—ãŸæŒ¨æ‹¶ã¯çœç•¥ã—ã¦OKã§ã™ã€‚**
        """

        response_text = ""
        used_model = "Gemini 2.5" # For logging
        footer_note = "" # Annotation for user

        try:
            # ---------------------------------------------------
            # â‘  Gemini 2.5 Flash (Main)
            # ---------------------------------------------------
            if not self.model_priority:
                 raise Exception("Gemini API Key missing")

            logger.info(f"âœ¨ 1. Gemini 2.5 Flash ã§æŒ‘æˆ¦ä¸­...")
            response = await self.model_priority.generate_content_async(prompt)
            response_text = response.text
        
        except Exception as e1:
            logger.warning(f"âš ï¸ Gemini 2.5 ã‚¨ãƒ©ãƒ¼: {e1}")
            try:
                # ---------------------------------------------------
                # â‘¡ Gemini 2.5 Flash Lite (Backup)
                # ---------------------------------------------------
                if not self.model_backup_1:
                     raise Exception("Gemini API Key missing")

                logger.info("â™»ï¸ 2. Gemini 2.5 Lite ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
                response = await self.model_backup_1.generate_content_async(prompt)
                response_text = response.text
                used_model = "Gemini Lite"
                footer_note = "\n\n(â€»çœã‚¨ãƒãƒ¢ãƒ¼ãƒ‰ğŸ”‹)"
                
            except Exception as e2:
                logger.warning(f"âš ï¸ Gemini Lite ã‚¨ãƒ©ãƒ¼: {e2}")
                # ---------------------------------------------------
                # â‘¢ Groq Llama 3 (Fallback)
                # ---------------------------------------------------
                if self.groq_client:
                    logger.info("ğŸ”¥ 3. Groq (Llama 3) å‡ºå‹•ï¼ï¼")
                    try:
                        # Call Groq API
                        completion = self.groq_client.chat.completions.create(
                            model="llama-3.3-70b-versatile", # High performance model
                            messages=[
                                # Inject system setting
                                {"role": "system", "content": CHARACTER_SETTING},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.7,
                            max_tokens=1024,
                        )
                        response_text = completion.choices[0].message.content or ""
                        used_model = "Groq Llama 3"
                        footer_note = "\n\n(â€»è¦åˆ¶ãƒ¢ãƒ¼ãƒ‰ğŸš€)"
                        logger.info("âœ… Groqã§ç”ŸæˆæˆåŠŸï¼")
                        
                    except Exception as e3:
                        logger.error(f"âŒ Groqã‚‚ã‚¨ãƒ©ãƒ¼: {e3}")
                        response_text = "ã”ã‚ã‚“ã­ã€ä»Šæ—¥ã¯å›ç·šãŒå…¨éƒ¨ãƒ‘ãƒ³ã‚¯ã—ã¡ã‚ƒã£ãŸã¿ãŸã„ğŸ˜µâ€ğŸ’«ğŸ’¦ ã¾ãŸæ˜æ—¥éŠã¼ã†ã­ï¼"
                else:
                    response_text = "ã”ã‚ã‚“ã­ã€ã¡ã‚‡ã£ã¨èª¿å­æ‚ªã„ã¿ãŸã„â€¦ğŸ’¦ (Groqã‚­ãƒ¼æœªè¨­å®š)"

        logger.info(f"ğŸ“¨ è¿”ä¿¡ãƒ¢ãƒ‡ãƒ«: {used_model}")
        
        # Add Dev Indicator
        if config.MAU_ENV == "development":
            footer_note += "\nğŸ› ï¸ (Dev Check)"

        return response_text + footer_note
